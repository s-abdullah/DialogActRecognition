{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile = \"TopTenTrain.csv\"\n",
    "save = \"posFeatures.csv\"\n",
    "\n",
    "saveFilets = \"TopTenTest.csv\"\n",
    "savets = \"posFeaturesTest.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATA\n"
     ]
    }
   ],
   "source": [
    "print(\"PROCESSING DATA\")\n",
    "data = pd.read_csv(saveFile, low_memory=False)\n",
    "datats = pd.read_csv(saveFilets, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting POS data\n",
    "pos = []\n",
    "for index, row in data.iterrows():\n",
    "    txt = row[\"cleanText\"]\n",
    "    pos.append(\" \".join([c[1] for c in (nltk.pos_tag(nltk.word_tokenize(txt)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting POS data\n",
    "posts = []\n",
    "for index, row in datats.iterrows():\n",
    "    txt = row[\"cleanText\"]\n",
    "    posts.append(\" \".join([c[1] for c in (nltk.pos_tag(nltk.word_tokenize(txt)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VBZ PRP VB NN', 'PRP VBP PRP RB VBZ', 'NN', 'RB', 'NN', 'PRP', 'PRP VBD IN PRP VBD', 'CC PRP MD RB', 'PRP VBP', 'NN']\n",
      "['NNP NN', 'RB JJ PRP PRP RB RB VBN IN NNP NNP', 'CC IN PRP VBD VBG IN DT NN IN NN NNS CC NNS', 'CC DT IN DT NNS WDT VBD JJ TO PRP IN WRB PRP$ NN VBD VBG IN PRP$ NN VBD JJ NNS PRP VBP', 'PRP', 'VBN VBN IN JJ NN IN PRP VBD VBN TO PRP$ RB VBG VBN IN DT', 'NN', '', 'RB PRP VBD PRP VBP JJ VBP PRP TO VB DT NN WDT MD VB PRP NN IN VB IN NN PDT DT NN CC VB TO VB IN IN DT NN CC', 'NNP NN']\n"
     ]
    }
   ],
   "source": [
    "print(pos[:10])\n",
    "print(posts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['pos','trees','ptb_treenumbers','WC','WPS','Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep','auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect','posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body','health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast','focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money','relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma','Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
    "\n",
    "data.insert(9, \"cleanPos\", pos, True)\n",
    "datats.insert(9, \"cleanPos\", posts, True)\n",
    "\n",
    "data = data.drop(drop, axis = 1)\n",
    "datats = datats.drop(drop, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147510\n",
      "26540\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(datats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMat = []\n",
    "\n",
    "for thing in pos:\n",
    "    vector = vectorizer.transform([thing])\n",
    "    posMat.append(vector.toarray()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMatts = []\n",
    "\n",
    "for thing in posts:\n",
    "    vector = vectorizer.transform([thing])\n",
    "    posMatts.append(vector.toarray()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vbz': 29, 'prp': 16, 'vb': 24, 'nn': 10, 'vbp': 28, 'rb': 17, 'vbd': 25, 'in': 5, 'cc': 0, 'md': 9, 'dt': 2, 'nns': 13, 'cd': 1, 'jj': 6, 'vbg': 26, 'to': 22, 'wp': 31, 'nnp': 11, 'uh': 23, 'wrb': 32, 'vbn': 27, 'jjr': 7, 'ex': 3, 'rp': 20, 'pdt': 14, 'jjs': 8, 'wdt': 30, 'rbr': 18, 'rbs': 19, 'fw': 4, 'nnps': 12, 'pos': 15, 'sym': 21}\n",
      "['cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'md', 'nn', 'nnp', 'nnps', 'nns', 'pdt', 'pos', 'prp', 'rb', 'rbr', 'rbs', 'rp', 'sym', 'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wrb']\n"
     ]
    }
   ],
   "source": [
    "ind = [0]*len((vectorizer.vocabulary_))\n",
    "print(vectorizer.vocabulary_)\n",
    "for thing in vectorizer.vocabulary_:\n",
    "    ind[vectorizer.vocabulary_[thing]] = thing\n",
    "\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147510, 33)\n",
      "(26540, 33)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thing = np.array(posMat)\n",
    "print(thing.shape)\n",
    "\n",
    "\n",
    "thingts = np.array(posMatts)\n",
    "print(thingts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(thing, columns=ind)\n",
    "final = pd.concat([data, df], axis = 1)\n",
    "final.to_csv(save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfts = pd.DataFrame(thingts, columns=ind)\n",
    "finalts = pd.concat([datats, dfts], axis = 1)\n",
    "finalts.to_csv(savets, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
