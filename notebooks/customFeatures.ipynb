{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile = \"TopTenTrain.csv\"\n",
    "save = \"customFeatures.csv\"\n",
    "\n",
    "saveFilets = \"TopTenTest.csv\"\n",
    "savets = \"customFeaturesTest.csv\"\n",
    "\n",
    "threshold =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATA\n"
     ]
    }
   ],
   "source": [
    "print(\"PROCESSING DATA\")\n",
    "data = pd.read_csv(saveFile, low_memory=False)\n",
    "datats = pd.read_csv(saveFilets, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['pos','trees','ptb_treenumbers','WC','WPS','Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep','auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect','posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body','health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast','focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money','relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma','Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
    "data = data.drop(drop, axis = 1)\n",
    "datats = datats.drop(drop, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customfeat(raw, clean ,filler):\n",
    "    # custom features vector of length 16\n",
    "    fv = [0]*16\n",
    "    # symbols in raw data\n",
    "    if \"?\" in raw:\n",
    "        fv[0] = 1\n",
    "        \n",
    "    if \"!\" in raw:\n",
    "        fv[1] = 1\n",
    "        \n",
    "    if \"--\" in raw:\n",
    "        fv[2] = 1\n",
    "        \n",
    "    if \"-\" in raw:\n",
    "        fv[3] = 1\n",
    "        \n",
    "    if \"+\" in raw:\n",
    "        fv[4] = 1\n",
    "        \n",
    "    if \"#\" in raw:\n",
    "        fv[5] = 1\n",
    "    # fillers\n",
    "    if \"F\" in filler:\n",
    "        fv[6] = 1\n",
    "        \n",
    "    if \"C\" in filler:\n",
    "        fv[7] = 1\n",
    "        \n",
    "    if \"D\" in filler:\n",
    "        fv[8] = 1\n",
    "        \n",
    "    if \"E\" in filler:\n",
    "        fv[9] = 1\n",
    "        \n",
    "    if \"A\" in filler:\n",
    "        fv[10] = 1\n",
    "        \n",
    "        \n",
    "    # length buckets\n",
    "    if len(clean) == 0 :\n",
    "        fv[11] = 1\n",
    "    else:\n",
    "        if len(clean) < 5:\n",
    "            fv[12] = 1\n",
    "\n",
    "        elif len(clean) < 15:\n",
    "            fv[13] = 1\n",
    "\n",
    "        elif len(clean) < 25:\n",
    "            fv[14] = 1\n",
    "        else:\n",
    "            fv[15] = 1\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making filler features\n",
    "\n",
    "action = []\n",
    "custom = []\n",
    "for index, row in data.iterrows():\n",
    "    txt = row[\"action\"]\n",
    "    custom.append(customfeat(row['text'], row['cleanText'], row['fillers']))\n",
    "    w = (txt.replace(\" \", \"_\"))\n",
    "    action.append((((w))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actionts = []\n",
    "customts = []\n",
    "for index, row in datats.iterrows():\n",
    "    txt = row[\"action\"]\n",
    "    customts.append(customfeat(row['text'], row['cleanText'], row['fillers']))\n",
    "    w = (txt.replace(\" \", \"_\"))\n",
    "    actionts.append((((w))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'None', 'beep_', 'long_pause_', 'None', 'None', 'None', 'laughter_', 'None', 'None']\n",
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
      "['None', 'None', 'None', 'None', 'None', 'laughter_', 'None', 'laughter_', 'None', 'None']\n",
      "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(action[:10])\n",
    "print(custom[:10])\n",
    "print(actionts[:10])\n",
    "print(customts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=threshold)\n",
    "vectorizer.fit(action)\n",
    "# print(len((vectorizer.vocabulary_)), vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMat = []\n",
    "\n",
    "for thing in action:\n",
    "    vector = vectorizer.transform([thing])\n",
    "    posMat.append(vector.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMatts = []\n",
    "\n",
    "for thing in actionts:\n",
    "    vector = vectorizer.transform([thing])\n",
    "    posMatts.append(vector.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147510, 58)\n",
      "(147510, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acti = np.array(posMat)\n",
    "print(acti.shape)\n",
    "\n",
    "cust = np.array(custom)\n",
    "print(cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26540, 58)\n",
      "(26540, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "actits = np.array(posMatts)\n",
    "print(actits.shape)\n",
    "\n",
    "custts = np.array(customts)\n",
    "print(custts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "['_laughter_', '_noise_', '_static_', '_talking_', '_tv_', 'baby_', 'baby_crying_', 'baby_talking_', 'barking_', 'beep_', 'breathing_', 'breathing_breathing_', 'breathing_laughter_', 'child_', 'child_crying_', 'child_talking_', 'child_yelling_', 'children_', 'children_talking_', 'clanging_', 'clicking_', 'cough_', 'dishes_', 'door_', 'faint_', 'faintly_', 'gasp_', 'inhaling_', 'laughter_', 'laughter_breathing_', 'laughter_laughter_', 'laughter_noise_', 'lipsmack_', 'lipsmack_breathing_', 'lipsmack_laughter_', 'mispronounced_', 'music_', 'noise_', 'noise_laughter_', 'none', 'pause_', 'ringing_', 'rustling_', 'sigh_', 'smack_', 'sneezing_', 'sniffing_', 'squeak_', 'static_', 'swallowing_', 'talking_', 'throat_clearing_', 'throat_clearing_laughter_', 'thumping_', 'tv_', 'very_faint_', 'very_faintly_', 'whistling_']\n"
     ]
    }
   ],
   "source": [
    "ind = [0]*len((vectorizer.vocabulary_))\n",
    "for thing in vectorizer.vocabulary_:\n",
    "    ind[vectorizer.vocabulary_[thing]] = thing\n",
    "print(len(ind))\n",
    "\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147510, 74)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(acti, columns=ind)\n",
    "df2 = pd.DataFrame(cust, columns=['?','!',\"--\", '-', '+', '#', 'F','C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf'])\n",
    "df = pd.concat([df1, df2], axis = 1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([data, df], axis = 1)\n",
    "final.to_csv(save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26540, 74)\n"
     ]
    }
   ],
   "source": [
    "df1ts = pd.DataFrame(actits, columns=ind)\n",
    "df2ts = pd.DataFrame(custts, columns=['?','!',\"--\", '-', '+', '#', 'F','C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf'])\n",
    "dfts = pd.concat([df1ts, df2ts], axis = 1)\n",
    "print(dfts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalts = pd.concat([datats, dfts], axis = 1)\n",
    "finalts.to_csv(savets, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
