{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATA\n",
      "(147510, 46)\n",
      "(26540, 46)\n",
      "PROCESSING DATA\n",
      "(147510, 86)\n",
      "(26540, 86)\n",
      "PROCESSING DATA\n",
      "(147510, 104)\n",
      "(26540, 104)\n",
      "PROCESSING DATA\n",
      "(147510, 714)\n",
      "(26540, 714)\n"
     ]
    }
   ],
   "source": [
    "print(\"PROCESSING DATA\")\n",
    "tstFilep = \"posFeaturesTest.csv\"\n",
    "baseFilep = \"posFeatures.csv\"\n",
    "trainFilep = pd.read_csv(baseFilep, low_memory=False)\n",
    "testFilep = pd.read_csv(tstFilep, low_memory=False)\n",
    "\n",
    "drop = ['swda_filename','ptb_basename','conversation_no','transcript_index','act_tag',\n",
    "        'caller','cleanText','action','fillers','utterance_index','subutterance_index','text']\n",
    "# trainFilep = trainFilep.drop(drop, axis = 1)\n",
    "# testFilep = testFilep.drop(drop, axis = 1)\n",
    "print(trainFilep.shape)\n",
    "print(testFilep.shape)\n",
    "\n",
    "\n",
    "print(\"PROCESSING DATA\")\n",
    "tstFilec = \"customFeaturesTest.csv\"\n",
    "baseFilec = \"customFeatures.csv\"\n",
    "trainFilec = pd.read_csv(baseFilec, low_memory=False)\n",
    "testFilec = pd.read_csv(tstFilec, low_memory=False)\n",
    "# trainFilec = trainFilec.drop(drop, axis = 1)\n",
    "# testFilec = testFilec.drop(drop, axis = 1)\n",
    "print(trainFilec.shape)\n",
    "print(testFilec.shape)\n",
    "\n",
    "print(\"PROCESSING DATA\")\n",
    "tstFile = \"TopTenTest.csv\"\n",
    "baseFile = \"TopTenTrain.csv\"\n",
    "trainFile = pd.read_csv(baseFile, low_memory=False)\n",
    "testFile = pd.read_csv(tstFile, low_memory=False)\n",
    "print(trainFile.shape)\n",
    "print(testFile.shape)\n",
    "\n",
    "print(\"PROCESSING DATA\")\n",
    "tstFileu = \"unigramFeaturesTest.csv\"\n",
    "baseFileu = \"unigramFeatures.csv\"\n",
    "trainFileu = pd.read_csv(baseFileu, low_memory=False)\n",
    "testFileu = pd.read_csv(tstFileu, low_memory=False)\n",
    "# trainFileu = trainFileu.drop(drop, axis = 1)\n",
    "# testFileu = testFileu.drop(drop, axis = 1)\n",
    "print(trainFileu.shape)\n",
    "print(testFileu.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFile = pd.concat([trainFile, trainFileu, trainFilep, trainFilec], axis = 1)\n",
    "# testFile = pd.concat([testFile, testFileu, testFilep, testFilec], axis = 1)\n",
    "# print(trainFile.shape)\n",
    "# print(testFile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current = 'sd'\n",
    "# current = 'b'\n",
    "# current = 'sv'\n",
    "# current = '%'\n",
    "# current = 'aa'\n",
    "# current = 'ba'\n",
    "# current = 'qy'\n",
    "# current = 'x'\n",
    "# current = 'ny'\n",
    "# current = 'fc'\n",
    "feats = ['sd', 'b', 'sv', '%', 'aa', 'ba', 'qy', 'x', 'ny', 'fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"PROCESSING DATA\")\n",
    "# tstFile = \"TopTenTest.csv\"\n",
    "# baseFile = \"TopTenTrain.csv\"\n",
    "# trainFile = pd.read_csv(baseFile, low_memory=False)\n",
    "# testFile = pd.read_csv(tstFile, low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline with only LIWC features:  ['Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body', 'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money', 'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
      "sd 0.8255463451394122\n",
      "b 0.9331574981160512\n",
      "sv 0.8539562923888471\n",
      "% 0.9412961567445366\n",
      "aa 0.9632629992464204\n",
      "ba 0.9760361718161266\n",
      "qy 0.9928409947249435\n",
      "x 0.9992464204973625\n",
      "ny 0.9844009042954032\n",
      "fc 0.9897889977392615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# features = ['WC','WPS']\n",
    "features = ['Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep',\n",
    "         'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect',\n",
    "         'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc',\n",
    "         'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body',\n",
    "         'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast',\n",
    "         'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money',\n",
    "         'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma',\n",
    "         'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
    "print(\"baseline with only LIWC features: \", features)\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFile[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFile[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in features:\n",
    "        feattr = np.array(trainFile[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFile[thing])\n",
    "        tester.append(featts)\n",
    "#     print(trainer[:2])\n",
    "#     print(tester.shape)\n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "    \n",
    "#     break\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(trainer, trtags)\n",
    "    print(current, clf.score(tester, tstags))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"PROCESSING DATA\")\n",
    "# tstFile = \"unigramFeaturesTest.csv\"\n",
    "# baseFile = \"unigramFeatures.csv\"\n",
    "# trainFile = pd.read_csv(baseFile, low_memory=False)\n",
    "# testFile = pd.read_csv(tstFile, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with only unigram features:  ['able', 'about', 'absolutely', 'actually', 'afford', 'after', 'again', 'against', 'age', 'ago', 'agree', 'ahead', 'air', 'all', 'almost', 'along', 'already', 'also', 'although', 'always', 'am', 'american', 'amount', 'an', 'and', 'another', 'any', 'anybody', 'anymore', 'anything', 'anyway', 'are', 'area', 'areas', 'arent', 'around', 'as', 'at', 'away', 'awful', 'baby', 'back', 'bad', 'basically', 'be', 'beautiful', 'because', 'become', 'been', 'before', 'being', 'believe', 'benefits', 'best', 'bet', 'better', 'between', 'big', 'bit', 'book', 'books', 'both', 'bought', 'boy', 'bring', 'brother', 'budget', 'built', 'business', 'but', 'buy', 'buying', 'by', 'bye', 'call', 'called', 'came', 'camping', 'can', 'cans', 'cant', 'capital', 'car', 'card', 'cards', 'care', 'cars', 'case', 'cases', 'catch', 'cause', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'check', 'child', 'children', 'city', 'class', 'close', 'cold', 'college', 'come', 'comes', 'coming', 'community', 'companies', 'company', 'computer', 'concerned', 'control', 'cook', 'cost', 'could', 'couldnt', 'countries', 'country', 'couple', 'course', 'credit', 'crime', 'cut', 'dallas', 'daughter', 'day', 'days', 'deal', 'death', 'decided', 'definitely', 'did', 'didnt', 'difference', 'different', 'difficult', 'dinner', 'do', 'does', 'doesnt', 'dog', 'doing', 'dollars', 'done', 'dont', 'door', 'down', 'drive', 'drug', 'drugs', 'during', 'each', 'early', 'easier', 'easy', 'eat', 'education', 'eight', 'eighty', 'either', 'else', 'end', 'ended', 'enjoy', 'enjoyed', 'enough', 'especially', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'exactly', 'example', 'except', 'exercise', 'expensive', 'experience', 'extra', 'fact', 'fairly', 'family', 'fan', 'far', 'father', 'favorite', 'feel', 'felt', 'few', 'fifteen', 'fifty', 'figure', 'finally', 'find', 'fine', 'first', 'fish', 'five', 'food', 'football', 'for', 'forth', 'forty', 'found', 'four', 'free', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'funny', 'game', 'gave', 'get', 'gets', 'getting', 'give', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goodness', 'gosh', 'got', 'gotten', 'government', 'great', 'grew', 'group', 'growing', 'guess', 'gun', 'guy', 'guys', 'had', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'has', 'hate', 'have', 'havent', 'having', 'he', 'health', 'hear', 'heard', 'help', 'her', 'here', 'hes', 'high', 'him', 'his', 'hit', 'home', 'homes', 'hope', 'hot', 'hour', 'hours', 'house', 'how', 'huh', 'hundred', 'husband', 'id', 'idea', 'if', 'ill', 'im', 'imagine', 'important', 'in', 'income', 'information', 'instead', 'insurance', 'interest', 'interested', 'interesting', 'into', 'involved', 'is', 'isnt', 'issue', 'it', 'its', 'ive', 'job', 'jury', 'just', 'keep', 'kept', 'kid', 'kids', 'kind', 'kinds', 'knew', 'know', 'large', 'last', 'late', 'lately', 'later', 'law', 'learn', 'least', 'leave', 'left', 'less', 'let', 'lets', 'level', 'life', 'like', 'liked', 'likes', 'line', 'listen', 'little', 'live', 'lived', 'lives', 'living', 'local', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'love', 'made', 'major', 'make', 'makes', 'making', 'man', 'many', 'married', 'matter', 'may', 'maybe', 'me', 'mean', 'men', 'middle', 'might', 'miles', 'mind', 'mine', 'minutes', 'money', 'month', 'months', 'more', 'morning', 'most', 'mostly', 'mother', 'move', 'moved', 'movie', 'movies', 'much', 'music', 'must', 'my', 'myself', 'name', 'neat', 'need', 'needed', 'needs', 'never', 'new', 'news', 'newspaper', 'next', 'nice', 'night', 'nine', 'nineteen', 'ninety', 'no', 'not', 'nothing', 'now', 'number', 'nursing', 'of', 'off', 'often', 'oh', 'okay', 'old', 'older', 'on', 'once', 'one', 'ones', 'only', 'or', 'other', 'ought', 'our', 'ours', 'out', 'outside', 'over', 'own', 'paid', 'paper', 'parents', 'part', 'particular', 'past', 'pay', 'paying', 'people', 'percent', 'person', 'phone', 'pick', 'place', 'places', 'plan', 'plano', 'play', 'playing', 'point', 'pretty', 'probably', 'problem', 'problems', 'program', 'programs', 'public', 'punishment', 'put', 'putting', 'question', 'quite', 'radio', 'rain', 'rather', 'read', 'reading', 'ready', 'real', 'realize', 'really', 'reason', 'recently', 'recycling', 'regular', 'remember', 'right', 'room', 'run', 'running', 'said', 'same', 'saw', 'say', 'saying', 'says', 'school', 'schools', 'second', 'see', 'seem', 'seems', 'seen', 'sell', 'send', 'sense', 'service', 'set', 'seven', 'seventy', 'several', 'she', 'shes', 'short', 'should', 'show', 'shows', 'side', 'since', 'sister', 'sit', 'sitting', 'situation', 'six', 'sixty', 'small', 'so', 'society', 'some', 'somebody', 'someone', 'something', 'sometimes', 'somewhere', 'son', 'soon', 'sort', 'sounds', 'spend', 'spent', 'start', 'started', 'starting', 'state', 'stay', 'still', 'story', 'stuff', 'such', 'summer', 'supposed', 'sure', 'system', 'take', 'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'tax', 'taxes', 'teach', 'teacher', 'teachers', 'team', 'television', 'tell', 'ten', 'tend', 'terms', 'test', 'testing', 'texas', 'th', 'than', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these', 'they', 'theyd', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'think', 'thinking', 'thirty', 'this', 'those', 'though', 'thought', 'thousand', 'three', 'through', 'throw', 'time', 'times', 'to', 'today', 'together', 'told', 'too', 'took', 'top', 'totally', 'tough', 'town', 'trees', 'tried', 'trouble', 'true', 'try', 'trying', 'turn', 'twelve', 'twenty', 'two', 'type', 'uh', 'um', 'under', 'understand', 'unless', 'until', 'up', 'us', 'use', 'used', 'using', 'usually', 'vacation', 'very', 'vote', 'wait', 'walk', 'want', 'wanted', 'wants', 'war', 'was', 'wasnt', 'watch', 'watching', 'water', 'way', 'ways', 'we', 'wear', 'weather', 'wed', 'week', 'weekend', 'weeks', 'well', 'went', 'were', 'werent', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whos', 'why', 'wife', 'will', 'winter', 'wish', 'with', 'within', 'without', 'woman', 'women', 'wonder', 'wonderful', 'wont', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worth', 'would', 'wouldnt', 'wow', 'wrong', 'yard', 'yeah', 'year', 'years', 'yep', 'yes', 'yet', 'you', 'youd', 'young', 'your', 'youre', 'yourself', 'youve']\n",
      "sd 0.8330821401657875\n",
      "b 0.9443481537302185\n",
      "sv 0.8684250188394875\n",
      "% 0.9273549359457423\n",
      "aa 0.9666164280331575\n",
      "ba 0.9785229841748304\n",
      "qy 0.9851168048229089\n",
      "x 0.9908063300678221\n",
      "ny 0.9844762622456669\n",
      "fc 0.9914845516201959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# features = ['WC','WPS']\n",
    "features = ['able', 'about', 'absolutely', 'actually', 'afford', 'after', 'again', 'against', 'age', 'ago',\n",
    "            'agree', 'ahead', 'air', 'all', 'almost', 'along', 'already', 'also', 'although', 'always', 'am',\n",
    "            'american', 'amount', 'an', 'and', 'another', 'any', 'anybody', 'anymore', 'anything', 'anyway',\n",
    "            'are', 'area', 'areas', 'arent', 'around', 'as', 'at', 'away', 'awful', 'baby', 'back', 'bad',\n",
    "            'basically', 'be', 'beautiful', 'because', 'become', 'been', 'before', 'being', 'believe',\n",
    "            'benefits', 'best', 'bet', 'better', 'between', 'big', 'bit', 'book', 'books', 'both',\n",
    "            'bought', 'boy', 'bring', 'brother', 'budget', 'built', 'business', 'but', 'buy', 'buying',\n",
    "            'by', 'bye', 'call', 'called', 'came', 'camping', 'can', 'cans', 'cant', 'capital', 'car', 'card',\n",
    "            'cards', 'care', 'cars', 'case', 'cases', 'catch', 'cause', 'certain', 'certainly', 'chance',\n",
    "            'change', 'changed', 'changes', 'check', 'child', 'children', 'city', 'class', 'close', 'cold',\n",
    "            'college', 'come', 'comes', 'coming', 'community', 'companies', 'company', 'computer', 'concerned',\n",
    "            'control', 'cook', 'cost', 'could', 'couldnt', 'countries', 'country', 'couple', 'course', 'credit',\n",
    "            'crime', 'cut', 'dallas', 'daughter', 'day', 'days', 'deal', 'death', 'decided', 'definitely', 'did',\n",
    "            'didnt', 'difference', 'different', 'difficult', 'dinner', 'do', 'does', 'doesnt', 'dog', 'doing',\n",
    "            'dollars', 'done', 'dont', 'door', 'down', 'drive', 'drug', 'drugs', 'during', 'each', 'early',\n",
    "            'easier', 'easy', 'eat', 'education', 'eight', 'eighty', 'either', 'else', 'end', 'ended', 'enjoy',\n",
    "            'enjoyed', 'enough', 'especially', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything',\n",
    "            'exactly', 'example', 'except', 'exercise', 'expensive', 'experience', 'extra', 'fact', 'fairly', \n",
    "            'family', 'fan', 'far', 'father', 'favorite', 'feel', 'felt', 'few', 'fifteen', 'fifty', 'figure',\n",
    "            'finally', 'find', 'fine', 'first', 'fish', 'five', 'food', 'football', 'for', 'forth', 'forty',\n",
    "            'found', 'four', 'free', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'funny', 'game',\n",
    "            'gave', 'get', 'gets', 'getting', 'give', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goodness',\n",
    "            'gosh', 'got', 'gotten', 'government', 'great', 'grew', 'group', 'growing', 'guess', 'gun', 'guy',\n",
    "            'guys', 'had', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'has', 'hate',\n",
    "            'have', 'havent', 'having', 'he', 'health', 'hear', 'heard', 'help', 'her', 'here', 'hes', 'high',\n",
    "            'him', 'his', 'hit', 'home', 'homes', 'hope', 'hot', 'hour', 'hours', 'house', 'how', 'huh',\n",
    "            'hundred', 'husband', 'id', 'idea', 'if', 'ill', 'im', 'imagine', 'important', 'in', 'income',\n",
    "            'information', 'instead', 'insurance', 'interest', 'interested', 'interesting', 'into', 'involved',\n",
    "            'is', 'isnt', 'issue', 'it', 'its', 'ive', 'job', 'jury', 'just', 'keep', 'kept', 'kid', 'kids',\n",
    "            'kind', 'kinds', 'knew', 'know', 'large', 'last', 'late', 'lately', 'later', 'law', 'learn', 'least',\n",
    "            'leave', 'left', 'less', 'let', 'lets', 'level', 'life', 'like', 'liked', 'likes', 'line', 'listen',\n",
    "            'little', 'live', 'lived', 'lives', 'living', 'local', 'long', 'longer', 'look', 'looked', 'looking',\n",
    "            'looks', 'lost', 'lot', 'lots', 'love', 'made', 'major', 'make', 'makes', 'making', 'man', 'many',\n",
    "            'married', 'matter', 'may', 'maybe', 'me', 'mean', 'men', 'middle', 'might', 'miles', 'mind', 'mine',\n",
    "            'minutes', 'money', 'month', 'months', 'more', 'morning', 'most', 'mostly', 'mother', 'move', 'moved',\n",
    "            'movie', 'movies', 'much', 'music', 'must', 'my', 'myself', 'name', 'neat', 'need', 'needed', 'needs',\n",
    "            'never', 'new', 'news', 'newspaper', 'next', 'nice', 'night', 'nine', 'nineteen', 'ninety', 'no',\n",
    "            'not', 'nothing', 'now', 'number', 'nursing', 'of', 'off', 'often', 'oh', 'okay', 'old', 'older',\n",
    "            'on', 'once', 'one', 'ones', 'only', 'or', 'other', 'ought', 'our', 'ours', 'out', 'outside', 'over',\n",
    "            'own', 'paid', 'paper', 'parents', 'part', 'particular', 'past', 'pay', 'paying', 'people', 'percent',\n",
    "            'person', 'phone', 'pick', 'place', 'places', 'plan', 'plano', 'play', 'playing', 'point', 'pretty',\n",
    "            'probably', 'problem', 'problems', 'program', 'programs', 'public', 'punishment', 'put', 'putting',\n",
    "            'question', 'quite', 'radio', 'rain', 'rather', 'read', 'reading', 'ready', 'real', 'realize', \n",
    "            'really', 'reason', 'recently', 'recycling', 'regular', 'remember', 'right', 'room', 'run', 'running',\n",
    "            'said', 'same', 'saw', 'say', 'saying', 'says', 'school', 'schools', 'second', 'see', 'seem',\n",
    "            'seems', 'seen', 'sell', 'send', 'sense', 'service', 'set', 'seven', 'seventy', 'several', 'she',\n",
    "            'shes', 'short', 'should', 'show', 'shows', 'side', 'since', 'sister', 'sit', 'sitting', 'situation',\n",
    "            'six', 'sixty', 'small', 'so', 'society', 'some', 'somebody', 'someone', 'something', 'sometimes',\n",
    "            'somewhere', 'son', 'soon', 'sort', 'sounds', 'spend', 'spent', 'start', 'started', 'starting',\n",
    "            'state', 'stay', 'still', 'story', 'stuff', 'such', 'summer', 'supposed', 'sure', 'system', 'take',\n",
    "            'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'tax', 'taxes', 'teach', 'teacher',\n",
    "            'teachers', 'team', 'television', 'tell', 'ten', 'tend', 'terms', 'test', 'testing', 'texas', 'th',\n",
    "            'than', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these',\n",
    "            'they', 'theyd', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'think', 'thinking', 'thirty',\n",
    "            'this', 'those', 'though', 'thought', 'thousand', 'three', 'through', 'throw', 'time', 'times', 'to',\n",
    "            'today', 'together', 'told', 'too', 'took', 'top', 'totally', 'tough', 'town', 'trees', 'tried',\n",
    "            'trouble', 'true', 'try', 'trying', 'turn', 'twelve', 'twenty', 'two', 'type', 'uh', 'um', 'under',\n",
    "            'understand', 'unless', 'until', 'up', 'us', 'use', 'used', 'using', 'usually', 'vacation', 'very',\n",
    "            'vote', 'wait', 'walk', 'want', 'wanted', 'wants', 'war', 'was', 'wasnt', 'watch', 'watching',\n",
    "            'water', 'way', 'ways', 'we', 'wear', 'weather', 'wed', 'week', 'weekend', 'weeks', 'well', 'went',\n",
    "            'were', 'werent', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'whether', 'which', 'while',\n",
    "            'who', 'whole', 'whos', 'why', 'wife', 'will', 'winter', 'wish', 'with', 'within', 'without',\n",
    "            'woman', 'women', 'wonder', 'wonderful', 'wont', 'work', 'worked', 'working', 'works', 'world',\n",
    "            'worse', 'worth', 'would', 'wouldnt', 'wow', 'wrong', 'yard', 'yeah', 'year', 'years', 'yep', 'yes',\n",
    "            'yet', 'you', 'youd', 'young', 'your', 'youre', 'yourself', 'youve']\n",
    "print(\"model with only unigram features: \", features)\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFileu[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFileu[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in features:\n",
    "        feattr = np.array(trainFileu[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFileu[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "    \n",
    "\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(trainer, trtags)\n",
    "    print(current, clf.score(tester, tstags))\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"PROCESSING DATA\")\n",
    "# tstFile = \"posFeaturesTest.csv\"\n",
    "# baseFile = \"posFeatures.csv\"\n",
    "# trainFile = pd.read_csv(baseFile, low_memory=False)\n",
    "# testFile = pd.read_csv(tstFile, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with only pos features:  ['cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'md', 'nn', 'nnp', 'nnps', 'nns', 'pdt', 'pos', 'prp', 'rb', 'rbr', 'rbs', 'rp', 'sym', 'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wrb']\n",
      "sd 0.7530143180105501\n",
      "b 0.9048605877920121\n",
      "sv 0.8450263752825923\n",
      "% 0.9141296156744536\n",
      "aa 0.9676337603617181\n",
      "ba 0.9719291635267521\n",
      "qy 0.9806330067822155\n",
      "x 0.9995478522984175\n",
      "ny 0.984438583270535\n",
      "fc 0.9853051996985682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# features = ['WC','WPS']\n",
    "features = ['cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'md', 'nn', 'nnp', \n",
    "            'nnps', 'nns', 'pdt', 'pos', 'prp', 'rb', 'rbr', 'rbs', 'rp', 'sym', \n",
    "            'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wrb']\n",
    "print(\"model with only pos features: \", features)\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFilep[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFilep[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in features:\n",
    "        feattr = np.array(trainFilep[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilep[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "    \n",
    "\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(trainer, trtags)\n",
    "    print(current, clf.score(tester, tstags))\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"PROCESSING DATA\")\n",
    "# tstFile = \"customFeaturesTest.csv\"\n",
    "# baseFile = \"customFeatures.csv\"\n",
    "# trainFile = pd.read_csv(baseFile, low_memory=False)\n",
    "# testFile = pd.read_csv(tstFile, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with only pos features:  ['_laughter_', '_noise_', '_static_', '_talking_', '_tv_', 'baby_', 'baby_crying_', 'baby_talking_', 'barking_', 'beep_', 'breathing_', 'breathing_breathing_', 'breathing_laughter_', 'child_', 'child_crying_', 'child_talking_', 'child_yelling_', 'children_', 'children_talking_', 'clanging_', 'clicking_', 'cough_', 'dishes_', 'door_', 'faint_', 'faintly_', 'gasp_', 'inhaling_', 'laughter_', 'laughter_breathing_', 'laughter_laughter_', 'laughter_noise_', 'lipsmack_', 'lipsmack_breathing_', 'lipsmack_laughter_', 'mispronounced_', 'music_', 'noise_', 'noise_laughter_', 'none', 'pause_', 'ringing_', 'rustling_', 'sigh_', 'smack_', 'sneezing_', 'sniffing_', 'squeak_', 'static_', 'swallowing_', 'talking_', 'throat_clearing_', 'throat_clearing_laughter_', 'thumping_', 'tv_', 'very_faint_', 'very_faintly_', 'whistling_', '?', '!', '--', '-', '+', '#', 'F', 'C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf']\n",
      "sd 0.7925395629238885\n",
      "b 0.8800678221552374\n",
      "sv 0.8435568952524491\n",
      "% 0.948492840994725\n",
      "aa 0.9676337603617181\n",
      "ba 0.9717784476262246\n",
      "qy 0.9931801055011303\n",
      "x 0.9972117558402411\n",
      "ny 0.984438583270535\n",
      "fc 0.9852675207234364\n"
     ]
    }
   ],
   "source": [
    "# features = ['WC','WPS']\n",
    "features = ['_laughter_', '_noise_', '_static_', '_talking_', '_tv_', 'baby_', 'baby_crying_', 'baby_talking_', 'barking_',\n",
    "            'beep_', 'breathing_', 'breathing_breathing_', 'breathing_laughter_', 'child_', 'child_crying_', 'child_talking_',\n",
    "            'child_yelling_', 'children_', 'children_talking_', 'clanging_', 'clicking_', 'cough_', 'dishes_', 'door_',\n",
    "            'faint_', 'faintly_', 'gasp_', 'inhaling_', 'laughter_', 'laughter_breathing_', 'laughter_laughter_',\n",
    "            'laughter_noise_', 'lipsmack_', 'lipsmack_breathing_', 'lipsmack_laughter_', 'mispronounced_', 'music_',\n",
    "            'noise_', 'noise_laughter_', 'none', 'pause_', 'ringing_', 'rustling_', 'sigh_', 'smack_', 'sneezing_', \n",
    "            'sniffing_', 'squeak_', 'static_', 'swallowing_', 'talking_', 'throat_clearing_', 'throat_clearing_laughter_',\n",
    "            'thumping_', 'tv_', 'very_faint_','very_faintly_', 'whistling_',\n",
    "            '?','!',\"--\", '-', '+', '#', 'F','C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf']\n",
    "print(\"model with only pos features: \", features)\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFilec[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFilec[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in features:\n",
    "        feattr = np.array(trainFilec[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilec[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "    \n",
    "\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(trainer, trtags)\n",
    "    print(current, clf.score(tester, tstags))\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with only pos features: \n",
      "sd 0.8556141672946496\n",
      "b 0.9499623210248681\n",
      "sv 0.8732102486812359\n",
      "% 0.9631122833458929\n",
      "aa 0.9659758854559156\n",
      "ba 0.9808967596081387\n",
      "qy 0.9939713639788997\n",
      "x 0.9997739261492088\n",
      "ny 0.9845516201959307\n",
      "fc 0.9921627731725697\n"
     ]
    }
   ],
   "source": [
    "# features = ['WC','WPS']\n",
    "featuresc = ['_laughter_', '_noise_', '_static_', '_talking_', '_tv_', 'baby_', 'baby_crying_', 'baby_talking_', 'barking_',\n",
    "            'beep_', 'breathing_', 'breathing_breathing_', 'breathing_laughter_', 'child_', 'child_crying_', 'child_talking_',\n",
    "            'child_yelling_', 'children_', 'children_talking_', 'clanging_', 'clicking_', 'cough_', 'dishes_', 'door_',\n",
    "            'faint_', 'faintly_', 'gasp_', 'inhaling_', 'laughter_', 'laughter_breathing_', 'laughter_laughter_',\n",
    "            'laughter_noise_', 'lipsmack_', 'lipsmack_breathing_', 'lipsmack_laughter_', 'mispronounced_', 'music_',\n",
    "            'noise_', 'noise_laughter_', 'none', 'pause_', 'ringing_', 'rustling_', 'sigh_', 'smack_', 'sneezing_', \n",
    "            'sniffing_', 'squeak_', 'static_', 'swallowing_', 'talking_', 'throat_clearing_', 'throat_clearing_laughter_',\n",
    "            'thumping_', 'tv_', 'very_faint_','very_faintly_', 'whistling_',\n",
    "            '?','!',\"--\", '-', '+', '#', 'F','C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf']\n",
    "\n",
    "\n",
    "featuresp = ['cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'md', 'nn', 'nnp', \n",
    "            'nnps', 'nns', 'pdt', 'pos', 'prp', 'rb', 'rbr', 'rbs', 'rp', 'sym', \n",
    "            'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wrb']\n",
    "\n",
    "\n",
    "featuresu = ['able', 'about', 'absolutely', 'actually', 'afford', 'after', 'again', 'against', 'age', 'ago',\n",
    "            'agree', 'ahead', 'air', 'all', 'almost', 'along', 'already', 'also', 'although', 'always', 'am',\n",
    "            'american', 'amount', 'an', 'and', 'another', 'any', 'anybody', 'anymore', 'anything', 'anyway',\n",
    "            'are', 'area', 'areas', 'arent', 'around', 'as', 'at', 'away', 'awful', 'baby', 'back', 'bad',\n",
    "            'basically', 'be', 'beautiful', 'because', 'become', 'been', 'before', 'being', 'believe',\n",
    "            'benefits', 'best', 'bet', 'better', 'between', 'big', 'bit', 'book', 'books', 'both',\n",
    "            'bought', 'boy', 'bring', 'brother', 'budget', 'built', 'business', 'but', 'buy', 'buying',\n",
    "            'by', 'bye', 'call', 'called', 'came', 'camping', 'can', 'cans', 'cant', 'capital', 'car', 'card',\n",
    "            'cards', 'care', 'cars', 'case', 'cases', 'catch', 'cause', 'certain', 'certainly', 'chance',\n",
    "            'change', 'changed', 'changes', 'check', 'child', 'children', 'city', 'class', 'close', 'cold',\n",
    "            'college', 'come', 'comes', 'coming', 'community', 'companies', 'company', 'computer', 'concerned',\n",
    "            'control', 'cook', 'cost', 'could', 'couldnt', 'countries', 'country', 'couple', 'course', 'credit',\n",
    "            'crime', 'cut', 'dallas', 'daughter', 'day', 'days', 'deal', 'death', 'decided', 'definitely', 'did',\n",
    "            'didnt', 'difference', 'different', 'difficult', 'dinner', 'do', 'does', 'doesnt', 'dog', 'doing',\n",
    "            'dollars', 'done', 'dont', 'door', 'down', 'drive', 'drug', 'drugs', 'during', 'each', 'early',\n",
    "            'easier', 'easy', 'eat', 'education', 'eight', 'eighty', 'either', 'else', 'end', 'ended', 'enjoy',\n",
    "            'enjoyed', 'enough', 'especially', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything',\n",
    "            'exactly', 'example', 'except', 'exercise', 'expensive', 'experience', 'extra', 'fact', 'fairly', \n",
    "            'family', 'fan', 'far', 'father', 'favorite', 'feel', 'felt', 'few', 'fifteen', 'fifty', 'figure',\n",
    "            'finally', 'find', 'fine', 'first', 'fish', 'five', 'food', 'football', 'for', 'forth', 'forty',\n",
    "            'found', 'four', 'free', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'funny', 'game',\n",
    "            'gave', 'get', 'gets', 'getting', 'give', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goodness',\n",
    "            'gosh', 'got', 'gotten', 'government', 'great', 'grew', 'group', 'growing', 'guess', 'gun', 'guy',\n",
    "            'guys', 'had', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'has', 'hate',\n",
    "            'have', 'havent', 'having', 'he', 'health', 'hear', 'heard', 'help', 'her', 'here', 'hes', 'high',\n",
    "            'him', 'his', 'hit', 'home', 'homes', 'hope', 'hot', 'hour', 'hours', 'house', 'how', 'huh',\n",
    "            'hundred', 'husband', 'id', 'idea', 'if', 'ill', 'im', 'imagine', 'important', 'in', 'income',\n",
    "            'information', 'instead', 'insurance', 'interest', 'interested', 'interesting', 'into', 'involved',\n",
    "            'is', 'isnt', 'issue', 'it', 'its', 'ive', 'job', 'jury', 'just', 'keep', 'kept', 'kid', 'kids',\n",
    "            'kind', 'kinds', 'knew', 'know', 'large', 'last', 'late', 'lately', 'later', 'law', 'learn', 'least',\n",
    "            'leave', 'left', 'less', 'let', 'lets', 'level', 'life', 'like', 'liked', 'likes', 'line', 'listen',\n",
    "            'little', 'live', 'lived', 'lives', 'living', 'local', 'long', 'longer', 'look', 'looked', 'looking',\n",
    "            'looks', 'lost', 'lot', 'lots', 'love', 'made', 'major', 'make', 'makes', 'making', 'man', 'many',\n",
    "            'married', 'matter', 'may', 'maybe', 'me', 'mean', 'men', 'middle', 'might', 'miles', 'mind', 'mine',\n",
    "            'minutes', 'money', 'month', 'months', 'more', 'morning', 'most', 'mostly', 'mother', 'move', 'moved',\n",
    "            'movie', 'movies', 'much', 'music', 'must', 'my', 'myself', 'name', 'neat', 'need', 'needed', 'needs',\n",
    "            'never', 'new', 'news', 'newspaper', 'next', 'nice', 'night', 'nine', 'nineteen', 'ninety', 'no',\n",
    "            'not', 'nothing', 'now', 'number', 'nursing', 'of', 'off', 'often', 'oh', 'okay', 'old', 'older',\n",
    "            'on', 'once', 'one', 'ones', 'only', 'or', 'other', 'ought', 'our', 'ours', 'out', 'outside', 'over',\n",
    "            'own', 'paid', 'paper', 'parents', 'part', 'particular', 'past', 'pay', 'paying', 'people', 'percent',\n",
    "            'person', 'phone', 'pick', 'place', 'places', 'plan', 'plano', 'play', 'playing', 'point', 'pretty',\n",
    "            'probably', 'problem', 'problems', 'program', 'programs', 'public', 'punishment', 'put', 'putting',\n",
    "            'question', 'quite', 'radio', 'rain', 'rather', 'read', 'reading', 'ready', 'real', 'realize', \n",
    "            'really', 'reason', 'recently', 'recycling', 'regular', 'remember', 'right', 'room', 'run', 'running',\n",
    "            'said', 'same', 'saw', 'say', 'saying', 'says', 'school', 'schools', 'second', 'see', 'seem',\n",
    "            'seems', 'seen', 'sell', 'send', 'sense', 'service', 'set', 'seven', 'seventy', 'several', 'she',\n",
    "            'shes', 'short', 'should', 'show', 'shows', 'side', 'since', 'sister', 'sit', 'sitting', 'situation',\n",
    "            'six', 'sixty', 'small', 'so', 'society', 'some', 'somebody', 'someone', 'something', 'sometimes',\n",
    "            'somewhere', 'son', 'soon', 'sort', 'sounds', 'spend', 'spent', 'start', 'started', 'starting',\n",
    "            'state', 'stay', 'still', 'story', 'stuff', 'such', 'summer', 'supposed', 'sure', 'system', 'take',\n",
    "            'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'tax', 'taxes', 'teach', 'teacher',\n",
    "            'teachers', 'team', 'television', 'tell', 'ten', 'tend', 'terms', 'test', 'testing', 'texas', 'th',\n",
    "            'than', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these',\n",
    "            'they', 'theyd', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'think', 'thinking', 'thirty',\n",
    "            'this', 'those', 'though', 'thought', 'thousand', 'three', 'through', 'throw', 'time', 'times', 'to',\n",
    "            'today', 'together', 'told', 'too', 'took', 'top', 'totally', 'tough', 'town', 'trees', 'tried',\n",
    "            'trouble', 'true', 'try', 'trying', 'turn', 'twelve', 'twenty', 'two', 'type', 'uh', 'um', 'under',\n",
    "            'understand', 'unless', 'until', 'up', 'us', 'use', 'used', 'using', 'usually', 'vacation', 'very',\n",
    "            'vote', 'wait', 'walk', 'want', 'wanted', 'wants', 'war', 'was', 'wasnt', 'watch', 'watching',\n",
    "            'water', 'way', 'ways', 'we', 'wear', 'weather', 'wed', 'week', 'weekend', 'weeks', 'well', 'went',\n",
    "            'were', 'werent', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'whether', 'which', 'while',\n",
    "            'who', 'whole', 'whos', 'why', 'wife', 'will', 'winter', 'wish', 'with', 'within', 'without',\n",
    "            'woman', 'women', 'wonder', 'wonderful', 'wont', 'work', 'worked', 'working', 'works', 'world',\n",
    "            'worse', 'worth', 'would', 'wouldnt', 'wow', 'wrong', 'yard', 'yeah', 'year', 'years', 'yep', 'yes',\n",
    "            'yet', 'you', 'youd', 'young', 'your', 'youre', 'yourself', 'youve']\n",
    "\n",
    "\n",
    "print(\"model with only pos features: \")\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFilec[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFilec[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in featuresu:\n",
    "        feattr = np.array(trainFileu[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFileu[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    for thing in featuresp:\n",
    "        feattr = np.array(trainFilep[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilep[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    for thing in featuresc:\n",
    "        feattr = np.array(trainFilec[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilec[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "               \n",
    "        \n",
    "        \n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "    \n",
    "\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(trainer, trtags)\n",
    "    print(current, clf.score(tester, tstags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with only pos features: \n",
      "[[ 8741  1546]\n",
      " [ 2065 14188]]\n",
      "sd 0.8639412207987943\n",
      "[[ 5716   441]\n",
      " [  899 19484]]\n",
      "b 0.9495101733232856\n",
      "[[ 1595  2557]\n",
      " [  738 21650]]\n",
      "sv 0.8758477769404672\n",
      "[[ 1798   642]\n",
      " [  277 23823]]\n",
      "% 0.9653730218538056\n",
      "[[  340   519]\n",
      " [  445 25236]]\n",
      "aa 0.9636774679728711\n",
      "[[  362   394]\n",
      " [  103 25681]]\n",
      "ba 0.9812735493594574\n",
      "[[  404   112]\n",
      " [   30 25994]]\n",
      "qy 0.9946495855312736\n",
      "[[  568     1]\n",
      " [    4 25967]]\n",
      "x 0.9998116051243406\n",
      "[[    9   404]\n",
      " [   10 26117]]\n",
      "ny 0.9844009042954032\n",
      "[[  205   186]\n",
      " [    5 26144]]\n",
      "fc 0.9928033157498116\n"
     ]
    }
   ],
   "source": [
    "# features = ['WC','WPS']\n",
    "featuresc = ['_laughter_', '_noise_', '_static_', '_talking_', '_tv_', 'baby_', 'baby_crying_', 'baby_talking_', 'barking_',\n",
    "            'beep_', 'breathing_', 'breathing_breathing_', 'breathing_laughter_', 'child_', 'child_crying_', 'child_talking_',\n",
    "            'child_yelling_', 'children_', 'children_talking_', 'clanging_', 'clicking_', 'cough_', 'dishes_', 'door_',\n",
    "            'faint_', 'faintly_', 'gasp_', 'inhaling_', 'laughter_', 'laughter_breathing_', 'laughter_laughter_',\n",
    "            'laughter_noise_', 'lipsmack_', 'lipsmack_breathing_', 'lipsmack_laughter_', 'mispronounced_', 'music_',\n",
    "            'noise_', 'noise_laughter_', 'none', 'pause_', 'ringing_', 'rustling_', 'sigh_', 'smack_', 'sneezing_', \n",
    "            'sniffing_', 'squeak_', 'static_', 'swallowing_', 'talking_', 'throat_clearing_', 'throat_clearing_laughter_',\n",
    "            'thumping_', 'tv_', 'very_faint_','very_faintly_', 'whistling_',\n",
    "            '?','!',\"--\", '-', '+', '#', 'F','C', 'D', 'E', 'A', 'l0', 'l5', 'l15', 'l25', 'linf']\n",
    "\n",
    "\n",
    "featuresp = ['cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'md', 'nn', 'nnp', \n",
    "            'nnps', 'nns', 'pdt', 'pos', 'prp', 'rb', 'rbr', 'rbs', 'rp', 'sym', \n",
    "            'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wrb']\n",
    "\n",
    "\n",
    "featuresu = ['able', 'about', 'absolutely', 'actually', 'afford', 'after', 'again', 'against', 'age', 'ago',\n",
    "            'agree', 'ahead', 'air', 'all', 'almost', 'along', 'already', 'also', 'although', 'always', 'am',\n",
    "            'american', 'amount', 'an', 'and', 'another', 'any', 'anybody', 'anymore', 'anything', 'anyway',\n",
    "            'are', 'area', 'areas', 'arent', 'around', 'as', 'at', 'away', 'awful', 'baby', 'back', 'bad',\n",
    "            'basically', 'be', 'beautiful', 'because', 'become', 'been', 'before', 'being', 'believe',\n",
    "            'benefits', 'best', 'bet', 'better', 'between', 'big', 'bit', 'book', 'books', 'both',\n",
    "            'bought', 'boy', 'bring', 'brother', 'budget', 'built', 'business', 'but', 'buy', 'buying',\n",
    "            'by', 'bye', 'call', 'called', 'came', 'camping', 'can', 'cans', 'cant', 'capital', 'car', 'card',\n",
    "            'cards', 'care', 'cars', 'case', 'cases', 'catch', 'cause', 'certain', 'certainly', 'chance',\n",
    "            'change', 'changed', 'changes', 'check', 'child', 'children', 'city', 'class', 'close', 'cold',\n",
    "            'college', 'come', 'comes', 'coming', 'community', 'companies', 'company', 'computer', 'concerned',\n",
    "            'control', 'cook', 'cost', 'could', 'couldnt', 'countries', 'country', 'couple', 'course', 'credit',\n",
    "            'crime', 'cut', 'dallas', 'daughter', 'day', 'days', 'deal', 'death', 'decided', 'definitely', 'did',\n",
    "            'didnt', 'difference', 'different', 'difficult', 'dinner', 'do', 'does', 'doesnt', 'dog', 'doing',\n",
    "            'dollars', 'done', 'dont', 'door', 'down', 'drive', 'drug', 'drugs', 'during', 'each', 'early',\n",
    "            'easier', 'easy', 'eat', 'education', 'eight', 'eighty', 'either', 'else', 'end', 'ended', 'enjoy',\n",
    "            'enjoyed', 'enough', 'especially', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything',\n",
    "            'exactly', 'example', 'except', 'exercise', 'expensive', 'experience', 'extra', 'fact', 'fairly', \n",
    "            'family', 'fan', 'far', 'father', 'favorite', 'feel', 'felt', 'few', 'fifteen', 'fifty', 'figure',\n",
    "            'finally', 'find', 'fine', 'first', 'fish', 'five', 'food', 'football', 'for', 'forth', 'forty',\n",
    "            'found', 'four', 'free', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'funny', 'game',\n",
    "            'gave', 'get', 'gets', 'getting', 'give', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goodness',\n",
    "            'gosh', 'got', 'gotten', 'government', 'great', 'grew', 'group', 'growing', 'guess', 'gun', 'guy',\n",
    "            'guys', 'had', 'half', 'hand', 'happen', 'happened', 'happens', 'happy', 'hard', 'has', 'hate',\n",
    "            'have', 'havent', 'having', 'he', 'health', 'hear', 'heard', 'help', 'her', 'here', 'hes', 'high',\n",
    "            'him', 'his', 'hit', 'home', 'homes', 'hope', 'hot', 'hour', 'hours', 'house', 'how', 'huh',\n",
    "            'hundred', 'husband', 'id', 'idea', 'if', 'ill', 'im', 'imagine', 'important', 'in', 'income',\n",
    "            'information', 'instead', 'insurance', 'interest', 'interested', 'interesting', 'into', 'involved',\n",
    "            'is', 'isnt', 'issue', 'it', 'its', 'ive', 'job', 'jury', 'just', 'keep', 'kept', 'kid', 'kids',\n",
    "            'kind', 'kinds', 'knew', 'know', 'large', 'last', 'late', 'lately', 'later', 'law', 'learn', 'least',\n",
    "            'leave', 'left', 'less', 'let', 'lets', 'level', 'life', 'like', 'liked', 'likes', 'line', 'listen',\n",
    "            'little', 'live', 'lived', 'lives', 'living', 'local', 'long', 'longer', 'look', 'looked', 'looking',\n",
    "            'looks', 'lost', 'lot', 'lots', 'love', 'made', 'major', 'make', 'makes', 'making', 'man', 'many',\n",
    "            'married', 'matter', 'may', 'maybe', 'me', 'mean', 'men', 'middle', 'might', 'miles', 'mind', 'mine',\n",
    "            'minutes', 'money', 'month', 'months', 'more', 'morning', 'most', 'mostly', 'mother', 'move', 'moved',\n",
    "            'movie', 'movies', 'much', 'music', 'must', 'my', 'myself', 'name', 'neat', 'need', 'needed', 'needs',\n",
    "            'never', 'new', 'news', 'newspaper', 'next', 'nice', 'night', 'nine', 'nineteen', 'ninety', 'no',\n",
    "            'not', 'nothing', 'now', 'number', 'nursing', 'of', 'off', 'often', 'oh', 'okay', 'old', 'older',\n",
    "            'on', 'once', 'one', 'ones', 'only', 'or', 'other', 'ought', 'our', 'ours', 'out', 'outside', 'over',\n",
    "            'own', 'paid', 'paper', 'parents', 'part', 'particular', 'past', 'pay', 'paying', 'people', 'percent',\n",
    "            'person', 'phone', 'pick', 'place', 'places', 'plan', 'plano', 'play', 'playing', 'point', 'pretty',\n",
    "            'probably', 'problem', 'problems', 'program', 'programs', 'public', 'punishment', 'put', 'putting',\n",
    "            'question', 'quite', 'radio', 'rain', 'rather', 'read', 'reading', 'ready', 'real', 'realize', \n",
    "            'really', 'reason', 'recently', 'recycling', 'regular', 'remember', 'right', 'room', 'run', 'running',\n",
    "            'said', 'same', 'saw', 'say', 'saying', 'says', 'school', 'schools', 'second', 'see', 'seem',\n",
    "            'seems', 'seen', 'sell', 'send', 'sense', 'service', 'set', 'seven', 'seventy', 'several', 'she',\n",
    "            'shes', 'short', 'should', 'show', 'shows', 'side', 'since', 'sister', 'sit', 'sitting', 'situation',\n",
    "            'six', 'sixty', 'small', 'so', 'society', 'some', 'somebody', 'someone', 'something', 'sometimes',\n",
    "            'somewhere', 'son', 'soon', 'sort', 'sounds', 'spend', 'spent', 'start', 'started', 'starting',\n",
    "            'state', 'stay', 'still', 'story', 'stuff', 'such', 'summer', 'supposed', 'sure', 'system', 'take',\n",
    "            'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'tax', 'taxes', 'teach', 'teacher',\n",
    "            'teachers', 'team', 'television', 'tell', 'ten', 'tend', 'terms', 'test', 'testing', 'texas', 'th',\n",
    "            'than', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these',\n",
    "            'they', 'theyd', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'think', 'thinking', 'thirty',\n",
    "            'this', 'those', 'though', 'thought', 'thousand', 'three', 'through', 'throw', 'time', 'times', 'to',\n",
    "            'today', 'together', 'told', 'too', 'took', 'top', 'totally', 'tough', 'town', 'trees', 'tried',\n",
    "            'trouble', 'true', 'try', 'trying', 'turn', 'twelve', 'twenty', 'two', 'type', 'uh', 'um', 'under',\n",
    "            'understand', 'unless', 'until', 'up', 'us', 'use', 'used', 'using', 'usually', 'vacation', 'very',\n",
    "            'vote', 'wait', 'walk', 'want', 'wanted', 'wants', 'war', 'was', 'wasnt', 'watch', 'watching',\n",
    "            'water', 'way', 'ways', 'we', 'wear', 'weather', 'wed', 'week', 'weekend', 'weeks', 'well', 'went',\n",
    "            'were', 'werent', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'whether', 'which', 'while',\n",
    "            'who', 'whole', 'whos', 'why', 'wife', 'will', 'winter', 'wish', 'with', 'within', 'without',\n",
    "            'woman', 'women', 'wonder', 'wonderful', 'wont', 'work', 'worked', 'working', 'works', 'world',\n",
    "            'worse', 'worth', 'would', 'wouldnt', 'wow', 'wrong', 'yard', 'yeah', 'year', 'years', 'yep', 'yes',\n",
    "            'yet', 'you', 'youd', 'young', 'your', 'youre', 'yourself', 'youve']\n",
    "\n",
    "\n",
    "features = ['Sixltr', 'Dic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep',\n",
    "         'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'affect',\n",
    "         'posemo', 'negemo', 'anx', 'anger', 'sad', 'social', 'family', 'friend', 'female', 'male', 'cogproc',\n",
    "         'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', 'body',\n",
    "         'health', 'sexual', 'ingest', 'drives', 'affiliation', 'achieve', 'power', 'reward', 'risk', 'focuspast',\n",
    "         'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'home', 'money',\n",
    "         'relig', 'death', 'informal', 'swear', 'netspeak', 'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma',\n",
    "         'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP']\n",
    "\n",
    "print(\"model with only pos features: \")\n",
    "\n",
    "\n",
    "for feat in feats:\n",
    "    current = feat\n",
    "\n",
    "    trtags = list(trainFilec[\"act_tag\"])\n",
    "    for x in range(len(trtags)):\n",
    "        t = trtags[x]\n",
    "        if t != current:\n",
    "            trtags[x] = 0\n",
    "        else:\n",
    "            trtags[x] = 1\n",
    "\n",
    "    trtags = np.array(trtags)\n",
    "#     print(trtags.shape)\n",
    "\n",
    "    tstags = list(testFilec[\"act_tag\"])\n",
    "    for x in range(len(tstags)):\n",
    "        t = tstags[x]\n",
    "        if t != current:\n",
    "            tstags[x] = 0\n",
    "        else:\n",
    "            tstags[x] = 1\n",
    "    tstags = np.array(tstags)\n",
    "#     print(tstags.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    trainer = []\n",
    "    tester = []\n",
    "    for thing in featuresu:\n",
    "        feattr = np.array(trainFileu[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFileu[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    for thing in featuresp:\n",
    "        feattr = np.array(trainFilep[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilep[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    for thing in featuresc:\n",
    "        feattr = np.array(trainFilec[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFilec[thing])\n",
    "        tester.append(featts)\n",
    "\n",
    "    for thing in features:\n",
    "        feattr = np.array(trainFile[thing])\n",
    "        trainer.append(feattr)\n",
    "\n",
    "        featts = np.array(testFile[thing])\n",
    "        tester.append(featts)\n",
    "             \n",
    "        \n",
    "        \n",
    "    trainer = np.array(trainer).T\n",
    "    tester = np.array(tester).T\n",
    "    \n",
    "\n",
    "#     print(trainer.shape)\n",
    "#     print(tester.shape)\n",
    "\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr', max_iter=5000).fit(trainer, trtags)\n",
    "    \n",
    "    y_pred = clf.predict(tester)\n",
    "\n",
    "    confusion = confusion_matrix(tstags,y_pred, labels=[1, 0])\n",
    "    print(confusion)\n",
    "    \n",
    "    print(current, clf.score(tester, tstags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
